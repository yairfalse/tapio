apiVersion: apps/v1
kind: Deployment
metadata:
  name: tapio-relay
  namespace: tapio-system
  labels:
    app: tapio-relay
    component: relay
    app.kubernetes.io/name: tapio-relay
    app.kubernetes.io/part-of: tapio
spec:
  replicas: 2  # HA setup with 2 replicas
  selector:
    matchLabels:
      app: tapio-relay
  template:
    metadata:
      labels:
        app: tapio-relay
        component: relay
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9096"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: tapio-relay
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - tapio-relay
              topologyKey: kubernetes.io/hostname
      containers:
      - name: relay
        image: tapio/tapio-relay:latest
        imagePullPolicy: Always
        command:
        - /tapio-relay
        args:
        - --engine=$(TAPIO_ENGINE_ENDPOINT)
        - --otel-endpoint=$(OTEL_ENDPOINT)
        - --buffer-size=200000  # Larger buffer for relay
        - --batch-size=2000
        env:
        - name: TAPIO_ENGINE_ENDPOINT
          value: "tapio-engine:9090"
        - name: OTEL_ENDPOINT
          value: "otel-collector:4317"
        - name: TAPIO_RELAY_LOG_LEVEL
          value: "info"
        ports:
        - name: grpc
          containerPort: 9095
          protocol: TCP
        - name: metrics
          containerPort: 9096
          protocol: TCP
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "2"
        livenessProbe:
          grpc:
            port: 9095
          initialDelaySeconds: 10
          periodSeconds: 30
        readinessProbe:
          grpc:
            port: 9095
          initialDelaySeconds: 5
          periodSeconds: 10
        volumeMounts:
        - name: config
          mountPath: /etc/tapio
          readOnly: true
      volumes:
      - name: config
        configMap:
          name: tapio-relay-config
          optional: true
---
apiVersion: v1
kind: Service
metadata:
  name: tapio-relay
  namespace: tapio-system
  labels:
    app: tapio-relay
    component: relay
spec:
  type: ClusterIP
  ports:
  - name: grpc
    port: 9095
    targetPort: 9095
    protocol: TCP
  - name: metrics
    port: 9096
    targetPort: 9096
    protocol: TCP
  selector:
    app: tapio-relay
---
# Headless service for direct pod access
apiVersion: v1
kind: Service
metadata:
  name: tapio-relay-headless
  namespace: tapio-system
  labels:
    app: tapio-relay
    component: relay
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: grpc
    port: 9095
    targetPort: 9095
    protocol: TCP
  selector:
    app: tapio-relay
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tapio-relay
  namespace: tapio-system
  labels:
    app: tapio-relay
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: tapio-relay
  labels:
    app: tapio-relay
rules:
- apiGroups: [""]
  resources: ["nodes", "pods", "services", "endpoints"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets", "daemonsets", "statefulsets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tapio-relay
  labels:
    app: tapio-relay
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tapio-relay
subjects:
- kind: ServiceAccount
  name: tapio-relay
  namespace: tapio-system
---
# HorizontalPodAutoscaler for automatic scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: tapio-relay
  namespace: tapio-system
  labels:
    app: tapio-relay
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: tapio-relay
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 minutes
      policies:
      - type: Percent
        value: 50  # Scale down by max 50% at a time
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60  # 1 minute
      policies:
      - type: Percent
        value: 100  # Can double the replicas
        periodSeconds: 60
      - type: Pods
        value: 4  # Or add 4 pods at a time
        periodSeconds: 60
      selectPolicy: Max  # Use the policy that allows the most scaling
---
# PodDisruptionBudget for high availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: tapio-relay
  namespace: tapio-system
  labels:
    app: tapio-relay
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: tapio-relay