name: Production Validation

on:
  # Trigger on production deployments
  deployment_status:
    types: [success]
  
  # Manual trigger
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      test_suites:
        description: 'Test suites to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - functional
          - performance
          - security
          - integration
          - e2e
      parallel:
        description: 'Run tests in parallel'
        required: false
        default: true
        type: boolean
      fail_fast:
        description: 'Stop on first failure'
        required: false
        default: false
        type: boolean

  # Scheduled runs
  schedule:
    # Nightly comprehensive tests
    - cron: '0 2 * * *'
    # Hourly smoke tests
    - cron: '0 * * * *'
    # Weekly security tests
    - cron: '0 6 * * 1'

  # Trigger on significant changes
  push:
    branches:
      - main
    paths:
      - 'pkg/**'
      - 'cmd/**'
      - 'deploy/**'
      - 'config/**'

env:
  # Global environment variables
  TAPIO_NAMESPACE: tapio-system
  GO_VERSION: '1.21'
  KUBECTL_VERSION: '1.28.0'
  HELM_VERSION: '3.12.0'

jobs:
  setup:
    name: Setup and Validation
    runs-on: ubuntu-latest
    outputs:
      environment: ${{ steps.config.outputs.environment }}
      test_suites: ${{ steps.config.outputs.test_suites }}
      parallel: ${{ steps.config.outputs.parallel }}
      fail_fast: ${{ steps.config.outputs.fail_fast }}
      should_run: ${{ steps.config.outputs.should_run }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure test parameters
        id: config
        run: |
          # Determine environment and test suites based on trigger
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
            echo "test_suites=${{ github.event.inputs.test_suites }}" >> $GITHUB_OUTPUT
            echo "parallel=${{ github.event.inputs.parallel }}" >> $GITHUB_OUTPUT
            echo "fail_fast=${{ github.event.inputs.fail_fast }}" >> $GITHUB_OUTPUT
            echo "should_run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "schedule" ]]; then
            case "${{ github.event.schedule }}" in
              "0 2 * * *")  # Nightly
                echo "environment=staging" >> $GITHUB_OUTPUT
                echo "test_suites=all" >> $GITHUB_OUTPUT
                echo "parallel=true" >> $GITHUB_OUTPUT
                echo "fail_fast=false" >> $GITHUB_OUTPUT
                ;;
              "0 * * * *")  # Hourly
                echo "environment=staging" >> $GITHUB_OUTPUT
                echo "test_suites=functional" >> $GITHUB_OUTPUT
                echo "parallel=true" >> $GITHUB_OUTPUT
                echo "fail_fast=true" >> $GITHUB_OUTPUT
                ;;
              "0 6 * * 1")  # Weekly security
                echo "environment=staging" >> $GITHUB_OUTPUT
                echo "test_suites=security" >> $GITHUB_OUTPUT
                echo "parallel=false" >> $GITHUB_OUTPUT
                echo "fail_fast=false" >> $GITHUB_OUTPUT
                ;;
            esac
            echo "should_run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "push" ]]; then
            echo "environment=staging" >> $GITHUB_OUTPUT
            echo "test_suites=functional security" >> $GITHUB_OUTPUT
            echo "parallel=true" >> $GITHUB_OUTPUT
            echo "fail_fast=true" >> $GITHUB_OUTPUT
            echo "should_run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "deployment_status" ]]; then
            if [[ "${{ github.event.deployment_status.state }}" == "success" ]]; then
              echo "environment=${{ github.event.deployment.environment }}" >> $GITHUB_OUTPUT
              echo "test_suites=functional integration" >> $GITHUB_OUTPUT
              echo "parallel=true" >> $GITHUB_OUTPUT
              echo "fail_fast=false" >> $GITHUB_OUTPUT
              echo "should_run=true" >> $GITHUB_OUTPUT
            else
              echo "should_run=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "should_run=false" >> $GITHUB_OUTPUT
          fi

      - name: Validate configuration
        if: steps.config.outputs.should_run == 'true'
        run: |
          echo "Environment: ${{ steps.config.outputs.environment }}"
          echo "Test Suites: ${{ steps.config.outputs.test_suites }}"
          echo "Parallel: ${{ steps.config.outputs.parallel }}"
          echo "Fail Fast: ${{ steps.config.outputs.fail_fast }}"
          
          # Validate test configuration exists
          if [[ ! -f "config/testing/validation.yaml" ]]; then
            echo "ERROR: Test configuration file not found"
            exit 1
          fi

  functional-tests:
    name: Functional Tests
    runs-on: ubuntu-latest
    needs: setup
    if: needs.setup.outputs.should_run == 'true' && contains(needs.setup.outputs.test_suites, 'functional') || contains(needs.setup.outputs.test_suites, 'all')
    timeout-minutes: 20
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: ${{ env.KUBECTL_VERSION }}

      - name: Setup test environment
        run: |
          # Install dependencies
          sudo apt-get update
          sudo apt-get install -y curl jq bc
          
          # Setup kubeconfig for testing
          mkdir -p ~/.kube
          echo "${{ secrets.KUBECONFIG_STAGING }}" | base64 -d > ~/.kube/config
          
          # Verify cluster access
          kubectl cluster-info

      - name: Run functional tests
        env:
          TAPIO_TEST_ENVIRONMENT: ${{ needs.setup.outputs.environment }}
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          ./tests/validation/run-tests.sh \
            --environment ${{ needs.setup.outputs.environment }} \
            --output /tmp/test-results \
            --verbose \
            functional

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: functional-test-results
          path: /tmp/test-results/
          retention-days: 30

      - name: Post test results
        if: always()
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const path = '/tmp/test-results/functional-results.json';
            
            if (fs.existsSync(path)) {
              const results = JSON.parse(fs.readFileSync(path, 'utf8'));
              
              const summary = `
              ## Functional Test Results
              
              - **Status**: ${results.status}
              - **Duration**: ${results.duration}
              - **Tests Run**: ${results.tests_run}
              - **Passed**: ${results.tests_passed}
              - **Failed**: ${results.tests_failed}
              - **Skipped**: ${results.tests_skipped}
              `;
              
              core.summary.addRaw(summary);
              await core.summary.write();
            }

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: setup
    if: needs.setup.outputs.should_run == 'true' && contains(needs.setup.outputs.test_suites, 'performance') || contains(needs.setup.outputs.test_suites, 'all')
    timeout-minutes: 45
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: ${{ env.KUBECTL_VERSION }}

      - name: Setup test environment
        run: |
          sudo apt-get update
          sudo apt-get install -y curl jq bc
          mkdir -p ~/.kube
          echo "${{ secrets.KUBECONFIG_STAGING }}" | base64 -d > ~/.kube/config

      - name: Run performance tests
        env:
          TAPIO_TEST_ENVIRONMENT: ${{ needs.setup.outputs.environment }}
        run: |
          ./tests/validation/run-tests.sh \
            --environment ${{ needs.setup.outputs.environment }} \
            --output /tmp/test-results \
            --verbose \
            performance

      - name: Analyze performance metrics
        run: |
          # Extract performance metrics
          if [[ -f "/tmp/test-results/performance-results.json" ]]; then
            jq '.metrics' /tmp/test-results/performance-results.json > /tmp/performance-metrics.json
            
            # Check against thresholds
            latency=$(jq -r '.latency_p99' /tmp/performance-metrics.json | sed 's/ms//')
            throughput=$(jq -r '.throughput' /tmp/performance-metrics.json)
            error_rate=$(jq -r '.error_rate' /tmp/performance-metrics.json)
            
            echo "Performance Metrics:"
            echo "- Latency P99: ${latency}ms"
            echo "- Throughput: ${throughput} req/s"
            echo "- Error Rate: ${error_rate}"
            
            # Validate against thresholds
            if (( $(echo "$latency > 10" | bc -l) )); then
              echo "WARNING: Latency exceeds 10ms threshold"
              exit 1
            fi
            
            if (( throughput < 10000 )); then
              echo "WARNING: Throughput below 10k req/s threshold"
              exit 1
            fi
          fi

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-results
          path: /tmp/test-results/
          retention-days: 30

  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    needs: setup
    if: needs.setup.outputs.should_run == 'true' && contains(needs.setup.outputs.test_suites, 'security') || contains(needs.setup.outputs.test_suites, 'all')
    timeout-minutes: 30
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: ${{ env.KUBECTL_VERSION }}

      - name: Install security tools
        run: |
          # Install Trivy for vulnerability scanning
          sudo apt-get update
          sudo apt-get install wget apt-transport-https gnupg lsb-release
          wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
          echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee -a /etc/apt/sources.list.d/trivy.list
          sudo apt-get update
          sudo apt-get install trivy

      - name: Setup test environment
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBECONFIG_STAGING }}" | base64 -d > ~/.kube/config

      - name: Run security tests
        env:
          TAPIO_TEST_ENVIRONMENT: ${{ needs.setup.outputs.environment }}
        run: |
          ./tests/validation/run-tests.sh \
            --environment ${{ needs.setup.outputs.environment }} \
            --output /tmp/test-results \
            --verbose \
            security

      - name: Run vulnerability scan
        run: |
          # Scan container images
          echo "Scanning Tapio container images..."
          
          # Get image names from Kubernetes
          kubectl get pods -n ${{ env.TAPIO_NAMESPACE }} -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\n' | sort -u > /tmp/images.txt
          
          # Scan each image
          while read -r image; do
            echo "Scanning image: $image"
            trivy image --format json --output "/tmp/scan-$(basename $image).json" "$image" || true
          done < /tmp/images.txt

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-test-results
          path: /tmp/test-results/
          retention-days: 30

      - name: Upload security scans
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-scans
          path: /tmp/scan-*.json
          retention-days: 30

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: setup
    if: needs.setup.outputs.should_run == 'true' && contains(needs.setup.outputs.test_suites, 'integration') || contains(needs.setup.outputs.test_suites, 'all')
    timeout-minutes: 30
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: ${{ env.KUBECTL_VERSION }}

      - name: Setup test environment
        run: |
          sudo apt-get update
          sudo apt-get install -y curl jq bc
          mkdir -p ~/.kube
          echo "${{ secrets.KUBECONFIG_STAGING }}" | base64 -d > ~/.kube/config

      - name: Run integration tests
        env:
          TAPIO_TEST_ENVIRONMENT: ${{ needs.setup.outputs.environment }}
        run: |
          ./tests/validation/run-tests.sh \
            --environment ${{ needs.setup.outputs.environment }} \
            --output /tmp/test-results \
            --verbose \
            integration

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: /tmp/test-results/
          retention-days: 30

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: [setup, functional-tests, integration-tests]
    if: needs.setup.outputs.should_run == 'true' && contains(needs.setup.outputs.test_suites, 'e2e') || contains(needs.setup.outputs.test_suites, 'all')
    timeout-minutes: 60
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: ${{ env.KUBECTL_VERSION }}

      - name: Setup test environment
        run: |
          sudo apt-get update
          sudo apt-get install -y curl jq bc
          mkdir -p ~/.kube
          echo "${{ secrets.KUBECONFIG_STAGING }}" | base64 -d > ~/.kube/config

      - name: Run E2E tests
        env:
          TAPIO_TEST_ENVIRONMENT: ${{ needs.setup.outputs.environment }}
        run: |
          ./tests/validation/run-tests.sh \
            --environment ${{ needs.setup.outputs.environment }} \
            --output /tmp/test-results \
            --verbose \
            e2e

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: e2e-test-results
          path: /tmp/test-results/
          retention-days: 30

  report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [setup, functional-tests, performance-tests, security-tests, integration-tests, e2e-tests]
    if: always() && needs.setup.outputs.should_run == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all test results
        uses: actions/download-artifact@v3
        with:
          path: /tmp/all-results

      - name: Generate comprehensive report
        run: |
          mkdir -p /tmp/final-report
          
          # Collect all test results
          find /tmp/all-results -name "*.json" -type f | while read -r file; do
            echo "Processing: $file"
            cp "$file" /tmp/final-report/
          done
          
          # Generate summary report
          ./tests/validation/run-tests.sh \
            --report-only \
            --output /tmp/final-report

      - name: Upload final report
        uses: actions/upload-artifact@v3
        with:
          name: comprehensive-test-report
          path: /tmp/final-report/
          retention-days: 90

      - name: Update status checks
        uses: actions/github-script@v6
        with:
          script: |
            // Update commit status based on test results
            const context = 'Production Validation';
            let state = 'success';
            let description = 'All production validation tests passed';
            
            // Check if any test jobs failed
            const jobs = [
              '${{ needs.functional-tests.result }}',
              '${{ needs.performance-tests.result }}',
              '${{ needs.security-tests.result }}',
              '${{ needs.integration-tests.result }}',
              '${{ needs.e2e-tests.result }}'
            ];
            
            if (jobs.includes('failure')) {
              state = 'failure';
              description = 'Some production validation tests failed';
            } else if (jobs.includes('cancelled')) {
              state = 'error';
              description = 'Production validation tests were cancelled';
            }
            
            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: state,
              description: description,
              context: context
            });

      - name: Notify stakeholders
        if: always()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          if [[ -n "$SLACK_WEBHOOK_URL" ]]; then
            # Determine overall status
            OVERALL_STATUS="success"
            if [[ "${{ needs.functional-tests.result }}" == "failure" ]] || \
               [[ "${{ needs.performance-tests.result }}" == "failure" ]] || \
               [[ "${{ needs.security-tests.result }}" == "failure" ]] || \
               [[ "${{ needs.integration-tests.result }}" == "failure" ]] || \
               [[ "${{ needs.e2e-tests.result }}" == "failure" ]]; then
              OVERALL_STATUS="failure"
            fi
            
            # Send notification
            curl -X POST -H 'Content-type: application/json' \
              --data "{
                \"text\": \"ðŸ§ª Tapio Production Validation Complete\",
                \"attachments\": [{
                  \"color\": \"$([ "$OVERALL_STATUS" == "success" ] && echo "good" || echo "danger")\",
                  \"fields\": [
                    {\"title\": \"Environment\", \"value\": \"${{ needs.setup.outputs.environment }}\", \"short\": true},
                    {\"title\": \"Status\", \"value\": \"$OVERALL_STATUS\", \"short\": true},
                    {\"title\": \"Trigger\", \"value\": \"${{ github.event_name }}\", \"short\": true},
                    {\"title\": \"Workflow\", \"value\": \"<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Results>\", \"short\": true}
                  ]
                }]
              }" \
              "$SLACK_WEBHOOK_URL"
          fi