---
# Centralized OTEL Collector Deployment for aggregation and export
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tapio-otel-collector
  namespace: tapio-system
  labels:
    app.kubernetes.io/name: tapio-otel-collector
    app.kubernetes.io/component: otel-collector
    app.kubernetes.io/part-of: tapio
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tapio-otel-collector
      app.kubernetes.io/component: otel-collector
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tapio-otel-collector
        app.kubernetes.io/component: otel-collector
        app.kubernetes.io/part-of: tapio
      annotations:
        checksum/config: "{{ include (print $.Template.BasePath \"/otel-configmap.yaml\") . | sha256sum }}"
        prometheus.io/scrape: "true"
        prometheus.io/port: "8888"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: tapio-otel-collector
      
      # Anti-affinity to spread replicas across nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values: ["tapio-otel-collector"]
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.96.0
        imagePullPolicy: IfNotPresent
        
        command: ["/otelcol-contrib"]
        args:
        - --config=/etc/otelcol-contrib/config.yaml
        - --feature-gates=-component.UseLocalHostAsDefaultHost
        
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: CLUSTER_NAME
          value: "production-cluster"
        
        securityContext:
          runAsUser: 10001
          runAsGroup: 10001
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi
        
        ports:
        - name: otlp-grpc
          containerPort: 4317
          protocol: TCP
        - name: otlp-http
          containerPort: 4318
          protocol: TCP
        - name: jaeger-grpc
          containerPort: 14250
          protocol: TCP
        - name: jaeger-thrift
          containerPort: 14268
          protocol: TCP
        - name: zipkin
          containerPort: 9411
          protocol: TCP
        - name: metrics
          containerPort: 8888
          protocol: TCP
        - name: pprof
          containerPort: 1777
          protocol: TCP
        - name: health-check
          containerPort: 13133
          protocol: TCP
        
        livenessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 15
          periodSeconds: 20
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 3
        
        volumeMounts:
        - name: config
          mountPath: /etc/otelcol-contrib
          readOnly: true
        - name: tmp
          mountPath: /tmp
        
      volumes:
      - name: config
        configMap:
          name: tapio-otel-collector-config
          defaultMode: 0644
      - name: tmp
        emptyDir:
          sizeLimit: 2Gi
      
      terminationGracePeriodSeconds: 60
---
# Service for centralized OTEL collector
apiVersion: v1
kind: Service
metadata:
  name: tapio-otel-collector
  namespace: tapio-system
  labels:
    app.kubernetes.io/name: tapio-otel-collector
    app.kubernetes.io/component: otel-collector
    app.kubernetes.io/part-of: tapio
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: tapio-otel-collector
    app.kubernetes.io/component: otel-collector
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
    protocol: TCP
  - name: otlp-http
    port: 4318
    targetPort: 4318
    protocol: TCP
  - name: jaeger-grpc
    port: 14250
    targetPort: 14250
    protocol: TCP
  - name: jaeger-thrift
    port: 14268
    targetPort: 14268
    protocol: TCP
  - name: zipkin
    port: 9411
    targetPort: 9411
    protocol: TCP
  - name: metrics
    port: 8888
    targetPort: 8888
    protocol: TCP
---
# Headless service for service discovery
apiVersion: v1
kind: Service
metadata:
  name: tapio-otel-collector-headless
  namespace: tapio-system
  labels:
    app.kubernetes.io/name: tapio-otel-collector
    app.kubernetes.io/component: otel-collector
    app.kubernetes.io/part-of: tapio
spec:
  type: ClusterIP
  clusterIP: None
  selector:
    app.kubernetes.io/name: tapio-otel-collector
    app.kubernetes.io/component: otel-collector
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
    protocol: TCP
  - name: otlp-http
    port: 4318
    targetPort: 4318
    protocol: TCP
---
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: tapio-otel-collector-hpa
  namespace: tapio-system
  labels:
    app.kubernetes.io/name: tapio-otel-collector
    app.kubernetes.io/component: otel-collector
    app.kubernetes.io/part-of: tapio
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: tapio-otel-collector
  minReplicas: 1
  maxReplicas: 2
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 2
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
---
# Pod Disruption Budget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: tapio-otel-collector-pdb
  namespace: tapio-system
  labels:
    app.kubernetes.io/name: tapio-otel-collector
    app.kubernetes.io/component: otel-collector
    app.kubernetes.io/part-of: tapio
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tapio-otel-collector
      app.kubernetes.io/component: otel-collector