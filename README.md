# Tapio ğŸŒ²

> **Work in Progress** - A simple Kubernetes debugging tool

Tapio is being developed to make Kubernetes debugging more accessible. Currently in early development with basic functionality working.

## What Works Today

### Basic Commands
```bash
# Check the health of your Kubernetes resources
tapio check

# Export metrics for Prometheus
tapio prometheus --port 8080

# Show version information
tapio version
```

### Current Features
- âœ… **Simple Health Checks** - Basic analysis of pods and deployments
- âœ… **Kubernetes Integration** - Works with your existing kubeconfig
- âœ… **Prometheus Metrics** - Exports health metrics for monitoring
- âœ… **Clean CLI** - Human-readable output, no configuration required

## Installation

### Prerequisites
- Kubernetes cluster access (if `kubectl get pods` works, Tapio will work)
- Go 1.21+ (for building from source)

### Build from Source
```bash
git clone https://github.com/your-org/tapio
cd tapio
make build
./bin/tapio check
```

## Example Output

```bash
$ tapio check
HEALTHY: 3 pods running normally
WARNING: 1 pod has resource issues
  - api-service: Memory usage at 85% of limit

READY: 2/3 deployments fully available
```

## Planned Features (Work in Progress)

We're working on adding more advanced capabilities:

- ğŸš§ **eBPF Integration** - Kernel-level insights for deeper debugging
- ğŸš§ **Advanced Correlation** - Connect Kubernetes events with system behavior  
- ğŸš§ **Predictive Analysis** - Early warning for potential issues
- ğŸš§ **Multi-layer Monitoring** - System, network, and application insights

## Architecture

### Current Architecture
Tapio is built with a modular, extensible foundation:

```
tapio/
â”œâ”€â”€ cmd/tapio/          # CLI entry point
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ k8s/           # Kubernetes client integration
â”‚   â”œâ”€â”€ simple/        # Basic health checking
â”‚   â”œâ”€â”€ metrics/       # Prometheus metrics
â”‚   â””â”€â”€ ebpf/          # eBPF framework (foundation ready)
â””â”€â”€ deploy/helm/       # Kubernetes deployment
```

### Future Vision: Complete System Intelligence

We're designing Tapio to eventually provide unprecedented visibility into Kubernetes systems. Here's the architecture we're working toward:

```
â”Œâ”€ User Experience â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ tapio check â†’ "Pod will OOM in 7 minutes"               â”‚
â”‚ tapio why   â†’ "Memory leak in /api/users endpoint"       â”‚
â”‚ tapio fix   â†’ "Applied memory limit increase"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€ Correlation Engine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Timeline analysis across all data sources             â”‚
â”‚ â€¢ Pattern recognition for known failure modes           â”‚
â”‚ â€¢ Confidence scoring for predictions                    â”‚
â”‚ â€¢ Human-readable root cause explanations                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€ Multi-Layer Data Collection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                          â”‚
â”‚ eBPF Layer (In Development)                             â”‚
â”‚ â”œâ”€ Memory: allocation tracking, leak detection          â”‚
â”‚ â”œâ”€ Network: packet analysis, connection mapping         â”‚
â”‚ â”œâ”€ Process: syscall patterns, resource usage            â”‚
â”‚ â””â”€ Performance: CPU scheduling, I/O bottlenecks         â”‚
â”‚                                                          â”‚
â”‚ System Layer (Planned)                                  â”‚
â”‚ â”œâ”€ systemd: service health, restart patterns           â”‚
â”‚ â”œâ”€ journald: log analysis, error correlation            â”‚
â”‚ â””â”€ container runtime: lifecycle events                  â”‚
â”‚                                                          â”‚
â”‚ Kubernetes Layer (Working Today) âœ…                     â”‚
â”‚ â”œâ”€ API resources: pods, deployments, services           â”‚
â”‚ â”œâ”€ Events: scheduling, failures, scaling                â”‚
â”‚ â””â”€ Metrics: resource usage, health status               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### The Big Idea: Predictive Kubernetes Debugging

Instead of reactive debugging ("why did it crash?"), we want to enable predictive insights:

**Today's Debugging:**
```bash
# Something breaks first, then you investigate
kubectl get pods  â†’ CrashLoopBackOff
kubectl logs pod  â†’ "OutOfMemoryError"
kubectl describe  â†’ "Container was OOMKilled"
```

**Tapio's Vision:**
```bash
# Catch problems before they happen
tapio check â†’ "WARNING: api-service will OOM in 7m23s"
tapio why   â†’ "Memory leak: 18MB/min growth in user session cache"
tapio fix   â†’ "Recommendation: Increase memory limit to 512Mi"
```

#### Technical Approach: Correlation Across Layers

The key insight is that Kubernetes problems usually have signatures across multiple system layers:

```
Problem: Memory Leak â†’ OOM â†’ Pod Restart
â”œâ”€ eBPF sees: Growing heap allocations, no corresponding frees
â”œâ”€ systemd sees: containerd memory pressure warnings  
â”œâ”€ journald sees: "Memory cgroup out of memory" messages
â””â”€ Kubernetes sees: OOMKilled event, pod restart

Tapio correlates these signals to predict the OOM before it happens
```

#### Why This Approach Could Work

1. **Layer Correlation**: Most K8s issues leave traces across multiple system layers
2. **Early Signals**: Kernel/system events often precede K8s-visible failures  
3. **Pattern Recognition**: Similar failure modes create recognizable signatures
4. **Explainable AI**: Rule-based correlation provides clear explanations

#### Development Philosophy

We're building this incrementally:
- âœ… **Foundation First**: Solid CLI, K8s integration, metrics (working today)
- ğŸš§ **Add Layers Gradually**: eBPF, then systemd, then advanced correlation
- ğŸ”® **Keep It Simple**: Complex backend, simple frontend ("just run `tapio check`")
- ğŸ“š **Learn and Iterate**: Real-world testing drives feature priorities

The goal isn't to replace existing tools, but to provide the "first command" you run when something seems wrong - the one that gives you the clearest picture of what's actually happening in your cluster.

## Development Status

### Completed Components
- [x] CLI framework with Cobra
- [x] Kubernetes API integration
- [x] Basic health analysis
- [x] Prometheus metrics export
- [x] Helm deployment charts
- [x] Cross-platform builds

### In Development
- [ ] eBPF-based system monitoring
- [ ] Advanced correlation engine
- [ ] systemd integration
- [ ] Network monitoring
- [ ] Performance optimization

## Contributing

This is an early-stage project and we welcome contributions! Areas where help is especially appreciated:

- Testing on different Kubernetes distributions
- eBPF program development
- Documentation improvements
- Feature suggestions and bug reports

### Development Setup
```bash
# Install development tools
make setup

# Run tests
make test

# Build and test locally
make build
./bin/tapio check
```

## Configuration

Tapio works with zero configuration by default. It uses your existing Kubernetes configuration from:
- `~/.kube/config`
- `KUBECONFIG` environment variable  
- In-cluster service account (when running as a pod)

## Deployment

### Kubernetes Deployment
```bash
# Deploy as a DaemonSet for cluster-wide monitoring
helm install tapio ./deploy/helm/tapio

# Or run locally
./bin/tapio check --all-namespaces
```

## Roadmap

### Short Term (Current Focus)
- Improve health analysis accuracy
- Add more Kubernetes resource types
- Enhanced error messages and debugging

### Medium Term
- eBPF integration for system-level insights
- Advanced correlation between K8s and system events
- Performance monitoring and predictions

### Long Term
- Machine learning for anomaly detection
- Integration with popular monitoring stacks
- Multi-cluster support

## Why Tapio?

Kubernetes debugging often requires deep expertise and multiple tools. Tapio aims to:

- **Simplify** - One command to understand what's wrong
- **Explain** - Clear, human-readable explanations
- **Predict** - Catch issues before they become problems
- **Integrate** - Work with existing tools and workflows

Named after the Finnish forest god, representing the deep roots needed to understand complex systems.

## License

MIT License - see [LICENSE](LICENSE) for details.

## Support

- **Issues**: [GitHub Issues](https://github.com/your-org/tapio/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-org/tapio/discussions)
- **Documentation**: [Wiki](https://github.com/your-org/tapio/wiki)

---

**Note**: This project is in active development. APIs and commands may change as we iterate toward v1.0. We appreciate your patience and feedback!
